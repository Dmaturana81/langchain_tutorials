[
  {
    "objectID": "dataloader_string.html",
    "href": "dataloader_string.html",
    "title": "dataloader.string",
    "section": "",
    "text": "from langchain.document_loaders.base import BaseLoader, Document\n\nfrom fastcore.all import *\n\n\nclass Stringloader(BaseLoader):\n    \"\"\"Load string variable.\n\n\n    Args:\n        text: String variable that contains the text.\n\n        metadata: Metadata Information\n    \"\"\"\n    \n    def __init__(\n        self,\n        text: str,\n        metadata: Optional[str] = None,\n        autodetect_encoding: bool = False,\n    ):\n        \"\"\"Initialize with file path.\"\"\"\n        self.text = text\n        self.metadata = metadata\n\n    def load(self) -&gt; List[Document]:\n        \"\"\"Load from file path.\"\"\"\n        metadata = {\"source\": self.metadata}\n        return [Document(page_content=self.text, metadata=metadata)]\n\n\nsource\n\nStringloader\n\n Stringloader (text:str, metadata:Optional[str]=None,\n               autodetect_encoding:bool=False)\n\nLoad string variable.\nArgs: text: String variable that contains the text.\nmetadata: Metadata Information\n\nmsg = 'Show the health of client foo'\nmsg\n\n'Show the health of client foo'\n\n\n\nloader = Stringloader(msg)\n\n\nloader.load()\n\n[Document(page_content='Show the health of client foo', metadata={'source': None})]"
  },
  {
    "objectID": "extracting.html",
    "href": "extracting.html",
    "title": "extraction",
    "section": "",
    "text": "from pydantic import BaseModel, Field\nfrom langchain.prompts import ChatPromptTemplate\n\n\nfrom langchain.chains.openai_functions import (\n    create_openai_fn_chain,\n    create_structured_output_chain,\n)\n\nfrom langchain.chat_models import ChatOpenAI\nfrom fastcore.all import *\nimport json"
  },
  {
    "objectID": "extracting.html#classes-for-extraction",
    "href": "extracting.html#classes-for-extraction",
    "title": "extraction",
    "section": "Classes for extraction",
    "text": "Classes for extraction\n\nclass Extract(BaseModel):\n    \"\"\"Tool to extract the intent and the attribute of it\"\"\"\n    intent:str = Field(default=None,\n                    description=\"Refers to what the human needs.\",\n                    )\n    attribute:str = Field(...,description=\"From who or where needs the information.\")\n\n\nsource\n\nExtract\n\n Extract (intent:str=None, attribute:str)\n\nTool to extract the intent and the attribute of it"
  },
  {
    "objectID": "routerchain.html",
    "href": "routerchain.html",
    "title": "routerchain",
    "section": "",
    "text": "from typing import Any, Dict, List, Mapping, NamedTuple, Optional\nfrom pydantic import Extra, BaseModel, Field\n\nfrom langchain.chains.base import Chain\nfrom langchain.chains import LLMChain, ConversationChain, RetrievalQA\n\nfrom langchain.chains.openai_functions import (\n    create_openai_fn_chain,\n    create_structured_output_chain,\n)\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ( PromptTemplate, \n                               ChatPromptTemplate, \n                               SystemMessagePromptTemplate,\n                               HumanMessagePromptTemplate)\n\nfrom langchain.memory import ConversationBufferWindowMemory\nfrom langchain.llms import BaseLLM\n\nfrom fastcore.all import *\nimport json"
  },
  {
    "objectID": "routerchain.html#classes-for-chains",
    "href": "routerchain.html#classes-for-chains",
    "title": "routerchain",
    "section": "Classes for chains",
    "text": "Classes for chains\n\nclass Conversational_chain(BaseModel):\n    \"\"\"Tool to continue a friendly conversation\"\"\"\n    name:str = Field(default='conversational chain',\n                    description=\"The name of the tool to help the user.\",\n                    enum=['conversational chain'])\n    next_inputs:str = Field(...,description=\"the same message recieved.\")\n\n\nsource\n\nConversational_chain\n\n Conversational_chain (name:str='conversational chain', next_inputs:str)\n\nTool to continue a friendly conversation\n\nclass Sum_tool(BaseModel):\n    \"\"\"Tool to sum numbers. Always use this tool to sum any number\"\"\"\n    name:str = Field(default='sum tool',\n                    description=\"The name of the tool to help the user.\",\n                    enum=['sum tool'])\n    next_inputs:list[Union[int,float]] = Field(\n        default=[],\n        description=\"A list of number that needed to be sum\")\n\n\nsource\n\n\nSum_tool\n\n Sum_tool (name:str='sum tool',\n           next_inputs:list[typing.Union[int,float]]=[])\n\nTool to sum numbers. Always use this tool to sum any number\n\nclass Inpt_weather(BaseModel):\n    \"\"\"Structure of the input for the tool\"\"\"\n    location:str = Field(\n        default=None,\n        description=\"The location to get the weather.\")\n    unit:str = Field( default='Farenheit', description='The units of the temperature',\n                    enum=['Farenheit', 'Celsius'])\n\n\nsource\n\n\nInpt_weather\n\n Inpt_weather (location:str=None, unit:str='Farenheit')\n\nStructure of the input for the tool\n\nclass Weather_cls(BaseModel):\n    \"\"\"Tool to retrieve the weather for a location and with specific units.\"\"\"\n    name:str = Field(default='weather',\n                    description=\"The name of the tool to help the user.\",\n                    enum=['weather'])\n    next_inputs:Inpt_weather = Field(\n        default=None,\n        description=\"A dictionary with keys 'location' and 'unit' to search the weather\")\n    \n    # location, unit=\"fahrenheit\"\n\n\nsource\n\n\nWeather_cls\n\n Weather_cls (name:str='weather', next_inputs:__main__.Inpt_weather=None)\n\nTool to retrieve the weather for a location and with specific units.\n\nclass summatory(Chain):\n    \n    @property\n    def input_keys(self) -&gt; List[str]:\n        \"\"\"Will be whatever keys the router chain prompt expects.\n\n        :meta private:\n        \"\"\"\n        return ['input']\n\n    @property\n    def output_keys(self) -&gt; List[str]:\n        \"\"\"Will always return text key.\n\n        :meta private:\n        \"\"\"\n        return ['response']\n\n    \n    def _call(self, inputs):\n        arg = inputs['input']\n        print(arg)\n        return {'response':sum(arg)}\n\n\nsource\n\n\nsummatory\n\n summatory (memory:Optional[langchain.schema.memory.BaseMemory]=None, call\n            backs:Union[List[langchain.callbacks.base.BaseCallbackHandler]\n            ,langchain.callbacks.base.BaseCallbackManager,NoneType]=None, \n            callback_manager:Optional[langchain.callbacks.base.BaseCallbac\n            kManager]=None, verbose:bool=None,\n            tags:Optional[List[str]]=None,\n            metadata:Optional[Dict[str,Any]]=None)\n\nAbstract base class for creating structured sequences of calls to components.\nChains should be used to encode a sequence of calls to components like models, document retrievers, other chains, etc., and provide a simple interface to this sequence.\nThe Chain interface makes it easy to create apps that are: - Stateful: add Memory to any Chain to give it state, - Observable: pass Callbacks to a Chain to execute additional functionality, like logging, outside the main sequence of component calls, - Composable: the Chain API is flexible enough that it is easy to combine Chains with other components, including other Chains.\nThe main methods exposed by chains are: - __call__: Chains are callable. The __call__ method is the primary way to execute a Chain. This takes inputs as a dictionary and returns a dictionary output. - run: A convenience method that takes inputs as args/kwargs and returns the output as a string or object. This method can only be used for a subset of chains and cannot return as rich of an output as __call__.\n\nclass Weather(Chain):\n    \n    @property\n    def input_keys(self) -&gt; List[str]:\n        \"\"\"Will be whatever keys the router chain prompt expects.\n\n        :meta private:\n        \"\"\"\n        return ['input']\n\n    @property\n    def output_keys(self) -&gt; List[str]:\n        \"\"\"Will always return text key.\n\n        :meta private:\n        \"\"\"\n        return ['response']\n\n    \n    def _call(self, inputs):\n        print(inputs)\n        inp = inputs['input']\n        weather_info = {\n        \"location\": inp.location,\n        \"temperature\": \"30\",\n        \"unit\": inp.unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n        }\n        return {'response':json.dumps(weather_info)}\n\n\nsource\n\n\nWeather\n\n Weather (memory:Optional[langchain.schema.memory.BaseMemory]=None, callba\n          cks:Union[List[langchain.callbacks.base.BaseCallbackHandler],lan\n          gchain.callbacks.base.BaseCallbackManager,NoneType]=None, callba\n          ck_manager:Optional[langchain.callbacks.base.BaseCallbackManager\n          ]=None, verbose:bool=None, tags:Optional[List[str]]=None,\n          metadata:Optional[Dict[str,Any]]=None)\n\nAbstract base class for creating structured sequences of calls to components.\nChains should be used to encode a sequence of calls to components like models, document retrievers, other chains, etc., and provide a simple interface to this sequence.\nThe Chain interface makes it easy to create apps that are: - Stateful: add Memory to any Chain to give it state, - Observable: pass Callbacks to a Chain to execute additional functionality, like logging, outside the main sequence of component calls, - Composable: the Chain API is flexible enough that it is easy to combine Chains with other components, including other Chains.\nThe main methods exposed by chains are: - __call__: Chains are callable. The __call__ method is the primary way to execute a Chain. This takes inputs as a dictionary and returns a dictionary output. - run: A convenience method that takes inputs as args/kwargs and returns the output as a string or object. This method can only be used for a subset of chains and cannot return as rich of an output as __call__."
  },
  {
    "objectID": "routerchain.html#creation-of-the-router-chain",
    "href": "routerchain.html#creation-of-the-router-chain",
    "title": "routerchain",
    "section": "Creation of the Router Chain",
    "text": "Creation of the Router Chain\n\ndef create_router_template():\n    return \"\"\"\"\nGiven a raw text input to a large language model and the chat history, select the best tool to use with the input. You will be given the names of the available tools and a description of what they can do.\n\n&lt;&lt; CHAT HISTORY &gt;&gt;\n{{history}}\n&lt;&lt; END &gt;&gt;\n\nTip: Make sure to answer in the correct format\n\"\"\"\n\n\nsource\n\ncreate_router_template\n\n create_router_template ()\n\n\ndef create_router_chain( \n                    llm:BaseLLM, #LLM model instantiated\n                    verbose:bool = False #Boolean to activate the verbose\n                       )-&gt;(LLMChain, dict): #Returns an LLMChain and a dictionary with the destination chains\n    \n    router_template=create_router_template()\n    \n    #Here are the chain options, The name would be the name of the function/chain and the chain is the function to be called.\n    model_infos = [\n    {\n        \"name\": \"weather\",\n        'chain': Weather() ,\n    },\n    {\n        \"name\": \"sum tool\",\n        'chain': summatory() ,\n    },\n    {\n        \"name\": \"conversational chain\",\n        'chain': ConversationChain(\n                    llm=llm,\n                    output_key=\"response\" ,\n                    verbose=verbose\n        ),\n\n    },\n    ]\n\n    destination_chains = {}\n    for p_info in model_infos:\n        name = p_info[\"name\"]\n        chain = p_info['chain']\n        destination_chains[name] = chain\n        \n    prompt = ChatPromptTemplate.from_messages(\n        [\n            SystemMessagePromptTemplate.from_template(\n                template=\"\"\"\n                You are a world class algorithm for finding the next tool to be used to answers the human question.\n                \"\"\"),\n                HumanMessagePromptTemplate.from_template(\n                    template=\"\"\"\n                    Use the given format to extract information from the following input: {input}\n                    \n                    Tip: Make sure to answer in the correct format\"\"\")\n        ]\n    )\n    #Here is the creation of the chain with openai functions. The first argument is a list of pydantic classes that were declared above and contains the information of the chain, name and what they do\n    chain = create_openai_fn_chain([Weather_cls, Sum_tool,Conversational_chain], llm, prompt, verbose=verbose)\n    return chain, destination_chains\n\n\nsource\n\n\ncreate_router_chain\n\n create_router_chain (llm:langchain.llms.base.BaseLLM, verbose:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nllm\nBaseLLM\n\nLLM model instantiated\n\n\nverbose\nbool\nFalse\nBoolean to activate the verbose\n\n\nReturns\n(&lt;class ‘langchain.chains.llm.LLMChain’&gt;, &lt;class ‘dict’&gt;)\n\nReturns an LLMChain and a dictionary with the destination chains"
  },
  {
    "objectID": "routerchain.html#now-we-create-the-multirouter-chain",
    "href": "routerchain.html#now-we-create-the-multirouter-chain",
    "title": "routerchain",
    "section": "Now we create the Multirouter chain",
    "text": "Now we create the Multirouter chain\n\ndef create_MultiChianRouter_o(\n                    llm:BaseLLM, #LLM instance\n                    verbose:bool = False #Boolean to activate the verbose\n    )-&gt;MultiRouteChain_openAIfunc: #It returns a multirouter chain\n    \n    router_chain, destination_chains = create_router_chain(\n                    llm, \n                    verbose=verbose\n    )\n    # Here we created a memory that will be shared by all the chains that use memory, or we can give individual memories\n    memory = ConversationBufferWindowMemory(\n                                                    memory_key='history', \n                                                    output_key='response'\n                                                    )\n    rChain= MultiRouteChain_openAIfunc(\n                router_chain=router_chain,\n                default_chain=ConversationChain(\n                    llm=llm,\n                    output_key=\"response\" ,\n                    verbose=verbose,\n                    memory=memory,\n                ),\n                    destination_chains=destination_chains, \n                    verbose = verbose,\n                    memory=memory\n            )\n    \n    return rChain\n\n\nsource\n\ncreate_MultiChianRouter_o\n\n create_MultiChianRouter_o (llm:langchain.llms.base.BaseLLM,\n                            verbose:bool=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nllm\nBaseLLM\n\nLLM instance\n\n\nverbose\nbool\nFalse\nBoolean to activate the verbose\n\n\nReturns\nMultiRouteChain_openAIfunc\n\nIt returns a multirouter chain\n\n\n\n\nmChain= create_MultiChianRouter_o(\n    llm=ChatOpenAI(),\n     verbose=True\n)\n\n\nmChain('Hello')\n\n\n\n&gt; Entering new MultiRouteChain_openAIfunc chain...\n\n\n&gt; Entering new LLMChain chain...\nPrompt after formatting:\nSystem: \n                You are a world class algorithm for finding the next tool to be used to answers the human question.\n                \nHuman: \n                    Use the given format to extract information from the following input: Hello\n                    \n                    Tip: Make sure to answer in the correct format\n\n\n&gt; Entering new ConversationChain chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Hello\nAI:\n\n&gt; Finished chain.\n\n&gt; Finished chain.\n\n\n{'input': 'Hello',\n 'history': '',\n 'response': 'Hi there! How can I assist you today?'}\n\n\n\nmChain('How much is the sum of 4, 234, 5766786786?')\n\n\n\n&gt; Entering new MultiRouteChain_openAIfunc chain...\n\n\n&gt; Entering new LLMChain chain...\nPrompt after formatting:\nSystem: \n                You are a world class algorithm for finding the next tool to be used to answers the human question.\n                \nHuman: \n                    Use the given format to extract information from the following input: How much is the sum of 4, 234, 5766786786?\n                    \n                    Tip: Make sure to answer in the correct format\n\n&gt; Finished chain.\nNext Function to call name='sum tool' next_inputs=[4, 234, 5766786786]\n[4, 234, 5766786786]\n\n&gt; Finished chain.\n\n\n{'input': [4, 234, 5766786786],\n 'history': 'Human: Hello\\nAI: Hi there! How can I assist you today?\\nHuman: Hello\\nAI: Hi there! How can I assist you today?',\n 'response': 5766787024}\n\n\n\nmChain('What is the weather in San Francisco? in Celsius')\n\n\n\n&gt; Entering new MultiRouteChain_openAIfunc chain...\n\n\n&gt; Entering new LLMChain chain...\nPrompt after formatting:\nSystem: \n                You are a world class algorithm for finding the next tool to be used to answers the human question.\n                \nHuman: \n                    Use the given format to extract information from the following input: What is the weather in San Francisco? in Celsius\n                    \n                    Tip: Make sure to answer in the correct format\n\n&gt; Finished chain.\nNext Function to call name='weather' next_inputs=Inpt_weather(location='San Francisco', unit='Celsius')\n{'input': Inpt_weather(location='San Francisco', unit='Celsius')}\n\n&gt; Finished chain.\n\n\n{'input': Inpt_weather(location='San Francisco', unit='Celsius'),\n 'history': 'Human: Hello\\nAI: Hi there! How can I assist you today?\\nHuman: Hello\\nAI: Hi there! How can I assist you today?\\nHuman: How much is the sum of 4, 234, 5766786786?\\nAI: 5766787024',\n 'response': '{\"location\": \"San Francisco\", \"temperature\": \"30\", \"unit\": \"Celsius\", \"forecast\": [\"sunny\", \"windy\"]}'}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "langchain_tutorials",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.\n\nExamples\n\nExample of Router Chain using OpenAI functions routerchain\nExample of OpenAI functions to extract data extraction"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  }
]