{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# routerchain\n",
    "\n",
    "> This module contains the development of a faster router chain using openai functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp routerchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from dotenv import load_dotenv, find_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "from typing import Any, Dict, List, Mapping, NamedTuple, Optional\n",
    "from pydantic import Extra, BaseModel, Field\n",
    "\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains import LLMChain, ConversationChain, RetrievalQA\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ( PromptTemplate, \n",
    "                               ChatPromptTemplate, \n",
    "                               SystemMessagePromptTemplate,\n",
    "                               HumanMessagePromptTemplate)\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms import BaseLLM\n",
    "\n",
    "from fastcore.all import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MutliRouteChain usinf OpenAI Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is the one that is routing the different chains, it receives the input, call the router chain.\n",
    "The Router chain will return the name of the next chain to call, if he doesn't find a next chain, it returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class MultiRouteChain_openAIfunc(Chain):\n",
    "    \"\"\"Use a single chain to route an input to one of multiple candidate chains.\"\"\"\n",
    "\n",
    "    router_chain: LLMChain\n",
    "    \"\"\"Chain that routes inputs to destination chains.\"\"\"\n",
    "    destination_chains: Mapping[str, Chain]\n",
    "    \"\"\"Chains that return final answer to inputs.\"\"\"\n",
    "    default_chain: Chain\n",
    "    \"\"\"Default chain to use when none of the destination chains are suitable.\"\"\"\n",
    "    silent_errors: bool = False\n",
    "    \"\"\"If True, use default_chain when an invalid destination name is provided. \n",
    "    Defaults to False.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        Extra.allow \n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the router chain prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return ['input']\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "    ) -> Dict[str, Any]:\n",
    "        try:\n",
    "            route =  self.router_chain(inputs)\n",
    "            route = route['function']\n",
    "            print(f\"Next Function to call {route}\")\n",
    "\n",
    "        except:\n",
    "            route = None\n",
    "            \n",
    "        if not route:\n",
    "            return self.default_chain(\n",
    "                {\n",
    "                    'input': inputs['input']\n",
    "                }\n",
    "            )\n",
    "        elif route.name in self.destination_chains:\n",
    "            return self.destination_chains[route.name](\n",
    "                route.next_inputs\n",
    "            )\n",
    "        elif self.silent_errors:\n",
    "            return self.default_chain(route.next_inputs, callbacks=callbacks)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid destination chain name '{route.destination}'\"\n",
    "            )\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "    ) -> Dict[str, Any]:\n",
    "        try:\n",
    "            route = self.router_chain.run(inputs)\n",
    "            route = route['function']\n",
    "            print(f\"Next Function to call {route}\")\n",
    "        except :\n",
    "            route = None\n",
    "\n",
    "        if not route:\n",
    "            return await self.default_chain.acall(\n",
    "                {\n",
    "                    'input': inputs['input']\n",
    "                }\n",
    "            )\n",
    "        elif route.name in self.destination_chains:\n",
    "            return await self.destination_chains[route.name].acall(\n",
    "                {\n",
    "                    'input':route.next_inputs\n",
    "                }\n",
    "            )\n",
    "        elif self.silent_errors:\n",
    "            return await self.default_chain.acall(\n",
    "                {\n",
    "                    'input': route.next_inputs\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid destination chain name '{route.destination}'\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Router chain declaration. It recieves the input and decide, using openAI functions, which would be the next chain to call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes for chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Conversational_chain(BaseModel):\n",
    "    \"\"\"Tool to continue a friendly conversation\"\"\"\n",
    "    name:str = Field(default='conversational chain',\n",
    "                    description=\"The name of the tool to help the user.\",\n",
    "                    enum=['conversational chain'])\n",
    "    next_inputs:str = Field(...,description=\"the same message recieved.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Retrieval_chain(BaseModel):\n",
    "    \"\"\"Tool to continue a friendly conversation\"\"\"\n",
    "    name:str = Field(default='retrieval chain',\n",
    "                    description=\"The name of the tool to help the user.\",\n",
    "                    enum=['retrieval chain'])\n",
    "    next_inputs:str = Field(...,description=\"A detailed question to search in a vectore database the answer to the question recieved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def create_router_template():\n",
    "    return \"\"\"\"\n",
    "Given a raw text input to a large language model and the chat history, select the best tool to use with the input. You will be given the names of the available tools and a description of what they can do.\n",
    "\n",
    "\n",
    "\n",
    "<< CHAT HISTORY >>\n",
    "{{history}}\n",
    "<< END >>\n",
    "\n",
    "Tip: Make sure to answer in the correct format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def create_router_chain( \n",
    "                    llm:BaseLLM, #LLM model instantiated\n",
    "                    verbose:bool = False #Boolean to activate the verbose\n",
    "                       )->(LLMChain, dict): #Returns an LLMChain and a dictionary with the destination chains\n",
    "    \n",
    "    router_template=create_router_template()\n",
    "    \n",
    "    #Here are the chain options, The name would be the name of the function/chain and the chain is the function to be called.\n",
    "    model_infos = [\n",
    "    # {\n",
    "    #     \"name\": \"retrieval chain\",\n",
    "    #     'chain': RetrievalQA.from_llm(llm=llm) ,\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"conversational chain\",\n",
    "        'chain': ConversationChain(\n",
    "                    llm=llm,\n",
    "                    output_key=\"response\" ,\n",
    "                    verbose=verbose\n",
    "        ),\n",
    "\n",
    "    },\n",
    "    ]\n",
    "\n",
    "    destination_chains = {}\n",
    "    for p_info in model_infos:\n",
    "        name = p_info[\"name\"]\n",
    "        chain = p_info['chain']\n",
    "        destination_chains[name] = chain\n",
    "        \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                template=\"\"\"\n",
    "                You are a world class algorithm for finding the next tool to be used to answers the human question.\n",
    "                \"\"\"),\n",
    "                HumanMessagePromptTemplate.from_template(\n",
    "                    template=\"\"\"\n",
    "                    Use the given format to extract information from the following input: {input}\n",
    "                    \n",
    "                    Tip: Make sure to answer in the correct format\"\"\")\n",
    "        ]\n",
    "    )\n",
    "    #Here is the creation of the chain with openai functions. The first argument is a list of pydantic classes that were declared above and contains the information of the chain, name and what they do\n",
    "    chain = create_openai_fn_chain([Conversational_chain], llm, prompt, verbose=verbose)\n",
    "    return chain, destination_chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we create the Multirouter chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def create_MultiChianRouter_o(\n",
    "                    llm:BaseLLM, #LLM instance\n",
    "                    verbose:bool = False #Boolean to activate the verbose\n",
    "    )->MultiRouteChain_openAIfunc: #It returns a multirouter chain\n",
    "    \n",
    "    router_chain, destination_chains = create_router_chain(\n",
    "                    llm, \n",
    "                    verbose=verbose\n",
    "    )\n",
    "    # Here we created a memory that will be shared by all the chains that use memory, or we can give individual memories\n",
    "    memory = ConversationBufferWindowMemory(\n",
    "                                                    memory_key='history', \n",
    "                                                    output_key='response'\n",
    "                                                    )\n",
    "    rChain= MultiRouteChain_openAIfunc(\n",
    "                router_chain=router_chain,\n",
    "                default_chain=ConversationChain(\n",
    "                    llm=llm,\n",
    "                    output_key=\"response\" ,\n",
    "                    verbose=verbose,\n",
    "                    memory=memory,\n",
    "                ),\n",
    "                    destination_chains=destination_chains, \n",
    "                    verbose = verbose,\n",
    "                    memory=memory\n",
    "            )\n",
    "    \n",
    "    return rChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "mChain= create_MultiChianRouter_o(\n",
    "    llm=ChatOpenAI(),\n",
    "     verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRouteChain_openAIfunc chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "                You are a world class algorithm for finding the next tool to be used to answers the human question.\n",
      "                \n",
      "Human: \n",
      "                    Use the given format to extract information from the following input: Hello\n",
      "                    \n",
      "                    Tip: Make sure to answer in the correct format\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Next Function to call name='conversational chain' next_inputs='Hello'\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello\n",
      "AI: Hi there! How can I assist you today?\n",
      "Human: Hello\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hello',\n",
       " 'history': 'Human: Hello\\nAI: Hi there! How can I assist you today?',\n",
       " 'response': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "mChain('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dasher_dev",
   "language": "python",
   "name": "dasher_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
