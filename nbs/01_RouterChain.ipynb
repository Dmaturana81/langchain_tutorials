{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# routerchain\n",
    "\n",
    "> This module contains the development of a faster router chain using openai functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp routerchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from dotenv import load_dotenv, find_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "from typing import Any, Dict, List, Mapping, NamedTuple, Optional\n",
    "from pydantic import Extra, BaseModel, Field\n",
    "\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains import LLMChain, ConversationChain, RetrievalQA\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ( PromptTemplate, \n",
    "                               ChatPromptTemplate, \n",
    "                               SystemMessagePromptTemplate,\n",
    "                               HumanMessagePromptTemplate)\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms import BaseLLM\n",
    "\n",
    "from fastcore.all import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MutliRouteChain usinf OpenAI Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is the one that is routing the different chains, it receives the input, call the router chain.\n",
    "The Router chain will return the name of the next chain to call, if he doesn't find a next chain, it returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class MultiRouteChain_openAIfunc(Chain):\n",
    "    \"\"\"Use a single chain to route an input to one of multiple candidate chains.\"\"\"\n",
    "\n",
    "    router_chain: LLMChain\n",
    "    \"\"\"Chain that routes inputs to destination chains.\"\"\"\n",
    "    destination_chains: Mapping[str, Chain]\n",
    "    \"\"\"Chains that return final answer to inputs.\"\"\"\n",
    "    default_chain: Chain\n",
    "    \"\"\"Default chain to use when none of the destination chains are suitable.\"\"\"\n",
    "    silent_errors: bool = False\n",
    "    \"\"\"If True, use default_chain when an invalid destination name is provided. \n",
    "    Defaults to False.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        Extra.allow \n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the router chain prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return ['input']\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "    ) -> Dict[str, Any]:\n",
    "        try:\n",
    "            route =  self.router_chain(inputs)\n",
    "            route = route['function']\n",
    "            print(f\"Next Function to call {route}\")\n",
    "\n",
    "        except:\n",
    "            route = None\n",
    "            \n",
    "        if not route:\n",
    "            return self.default_chain(\n",
    "                {\n",
    "                    'input': inputs['input']\n",
    "                }\n",
    "            )\n",
    "        elif route.name in self.destination_chains:\n",
    "            return self.destination_chains[route.name](\n",
    "                route.next_inputs\n",
    "            )\n",
    "        elif self.silent_errors:\n",
    "            return self.default_chain(route.next_inputs, callbacks=callbacks)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid destination chain name '{route.destination}'\"\n",
    "            )\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "    ) -> Dict[str, Any]:\n",
    "        try:\n",
    "            route = self.router_chain.run(inputs)\n",
    "            route = route['function']\n",
    "            print(f\"Next Function to call {route}\")\n",
    "        except :\n",
    "            route = None\n",
    "\n",
    "        if not route:\n",
    "            return await self.default_chain.acall(\n",
    "                {\n",
    "                    'input': inputs['input']\n",
    "                }\n",
    "            )\n",
    "        elif route.name in self.destination_chains:\n",
    "            return await self.destination_chains[route.name].acall(\n",
    "                {\n",
    "                    'input':route.next_inputs\n",
    "                }\n",
    "            )\n",
    "        elif self.silent_errors:\n",
    "            return await self.default_chain.acall(\n",
    "                {\n",
    "                    'input': route.next_inputs\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Received invalid destination chain name '{route.destination}'\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Router chain declaration. It recieves the input and decide, using openAI functions, which would be the next chain to call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes for chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Conversational_chain(BaseModel):\n",
    "    \"\"\"Tool to continue a friendly conversation\"\"\"\n",
    "    name:str = Field(default='conversational chain',\n",
    "                    description=\"The name of the tool to help the user.\",\n",
    "                    enum=['conversational chain'])\n",
    "    next_inputs:str = Field(...,description=\"the same message recieved.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Sum_tool(BaseModel):\n",
    "    \"\"\"Tool to sum numbers. Always use this tool to sum any number\"\"\"\n",
    "    name:str = Field(default='sum tool',\n",
    "                    description=\"The name of the tool to help the user.\",\n",
    "                    enum=['sum tool'])\n",
    "    next_inputs:list[Union[int,float]] = Field(\n",
    "        default=[],\n",
    "        description=\"A list of number that needed to be sum\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Inpt_weather(BaseModel):\n",
    "    \"\"\"Structure of the input for the tool\"\"\"\n",
    "    location:str = Field(\n",
    "        default=None,\n",
    "        description=\"The location to get the weather.\")\n",
    "    unit:str = Field( default='Farenheit', description='The units of the temperature',\n",
    "                    enum=['Farenheit', 'Celsius'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Weather_cls(BaseModel):\n",
    "    \"\"\"Tool to retrieve the weather for a location and with specific units.\"\"\"\n",
    "    name:str = Field(default='weather',\n",
    "                    description=\"The name of the tool to help the user.\",\n",
    "                    enum=['weather'])\n",
    "    next_inputs:Inpt_weather = Field(\n",
    "        default=None,\n",
    "        description=\"A dictionary with keys 'location' and 'unit' to search the weather\")\n",
    "    \n",
    "    # location, unit=\"fahrenheit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class summatory(Chain):\n",
    "    \n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the router chain prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return ['input']\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return ['response']\n",
    "\n",
    "    \n",
    "    def _call(self, inputs):\n",
    "        arg = inputs['input']\n",
    "        print(arg)\n",
    "        return {'response':sum(arg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Weather(Chain):\n",
    "    \n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the router chain prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return ['input']\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return ['response']\n",
    "\n",
    "    \n",
    "    def _call(self, inputs):\n",
    "        print(inputs)\n",
    "        inp = inputs['input']\n",
    "        weather_info = {\n",
    "        \"location\": inp.location,\n",
    "        \"temperature\": \"30\",\n",
    "        \"unit\": inp.unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "        }\n",
    "        return {'response':json.dumps(weather_info)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def create_router_template():\n",
    "    return \"\"\"\"\n",
    "Given a raw text input to a large language model and the chat history, select the best tool to use with the input. You will be given the names of the available tools and a description of what they can do.\n",
    "\n",
    "<< CHAT HISTORY >>\n",
    "{{history}}\n",
    "<< END >>\n",
    "\n",
    "Tip: Make sure to answer in the correct format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def create_router_chain( \n",
    "                    llm:BaseLLM, #LLM model instantiated\n",
    "                    verbose:bool = False #Boolean to activate the verbose\n",
    "                       )->(LLMChain, dict): #Returns an LLMChain and a dictionary with the destination chains\n",
    "    \n",
    "    router_template=create_router_template()\n",
    "    \n",
    "    #Here are the chain options, The name would be the name of the function/chain and the chain is the function to be called.\n",
    "    model_infos = [\n",
    "    {\n",
    "        \"name\": \"weather\",\n",
    "        'chain': Weather() ,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sum tool\",\n",
    "        'chain': summatory() ,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"conversational chain\",\n",
    "        'chain': ConversationChain(\n",
    "                    llm=llm,\n",
    "                    output_key=\"response\" ,\n",
    "                    verbose=verbose\n",
    "        ),\n",
    "\n",
    "    },\n",
    "    ]\n",
    "\n",
    "    destination_chains = {}\n",
    "    for p_info in model_infos:\n",
    "        name = p_info[\"name\"]\n",
    "        chain = p_info['chain']\n",
    "        destination_chains[name] = chain\n",
    "        \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                template=\"\"\"\n",
    "                You are a world class algorithm for finding the next tool to be used to answers the human question.\n",
    "                \"\"\"),\n",
    "                HumanMessagePromptTemplate.from_template(\n",
    "                    template=\"\"\"\n",
    "                    Use the given format to extract information from the following input: {input}\n",
    "                    \n",
    "                    Tip: Make sure to answer in the correct format\"\"\")\n",
    "        ]\n",
    "    )\n",
    "    #Here is the creation of the chain with openai functions. The first argument is a list of pydantic classes that were declared above and contains the information of the chain, name and what they do\n",
    "    chain = create_openai_fn_chain([Weather_cls, Sum_tool,Conversational_chain], llm, prompt, verbose=verbose)\n",
    "    return chain, destination_chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we create the Multirouter chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def create_MultiChianRouter_o(\n",
    "                    llm:BaseLLM, #LLM instance\n",
    "                    verbose:bool = False #Boolean to activate the verbose\n",
    "    )->MultiRouteChain_openAIfunc: #It returns a multirouter chain\n",
    "    \n",
    "    router_chain, destination_chains = create_router_chain(\n",
    "                    llm, \n",
    "                    verbose=verbose\n",
    "    )\n",
    "    # Here we created a memory that will be shared by all the chains that use memory, or we can give individual memories\n",
    "    memory = ConversationBufferWindowMemory(\n",
    "                                                    memory_key='history', \n",
    "                                                    output_key='response'\n",
    "                                                    )\n",
    "    rChain= MultiRouteChain_openAIfunc(\n",
    "                router_chain=router_chain,\n",
    "                default_chain=ConversationChain(\n",
    "                    llm=llm,\n",
    "                    output_key=\"response\" ,\n",
    "                    verbose=verbose,\n",
    "                    memory=memory,\n",
    "                ),\n",
    "                    destination_chains=destination_chains, \n",
    "                    verbose = verbose,\n",
    "                    memory=memory\n",
    "            )\n",
    "    \n",
    "    return rChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "mChain= create_MultiChianRouter_o(\n",
    "    llm=ChatOpenAI(),\n",
    "     verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRouteChain_openAIfunc chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "                You are a world class algorithm for finding the next tool to be used to answers the human question.\n",
      "                \n",
      "Human: \n",
      "                    Use the given format to extract information from the following input: Hello\n",
      "                    \n",
      "                    Tip: Make sure to answer in the correct format\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hello',\n",
       " 'history': '',\n",
       " 'response': 'Hi there! How can I assist you today?'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "mChain('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRouteChain_openAIfunc chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "                You are a world class algorithm for finding the next tool to be used to answers the human question.\n",
      "                \n",
      "Human: \n",
      "                    Use the given format to extract information from the following input: How much is the sum of 4, 234, 5766786786?\n",
      "                    \n",
      "                    Tip: Make sure to answer in the correct format\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Next Function to call name='sum tool' next_inputs=[4, 234, 5766786786]\n",
      "[4, 234, 5766786786]\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': [4, 234, 5766786786],\n",
       " 'history': 'Human: Hello\\nAI: Hi there! How can I assist you today?\\nHuman: Hello\\nAI: Hi there! How can I assist you today?',\n",
       " 'response': 5766787024}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "mChain('How much is the sum of 4, 234, 5766786786?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRouteChain_openAIfunc chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "                You are a world class algorithm for finding the next tool to be used to answers the human question.\n",
      "                \n",
      "Human: \n",
      "                    Use the given format to extract information from the following input: What is the weather in San Francisco? in Celsius\n",
      "                    \n",
      "                    Tip: Make sure to answer in the correct format\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Next Function to call name='weather' next_inputs=Inpt_weather(location='San Francisco', unit='Celsius')\n",
      "{'input': Inpt_weather(location='San Francisco', unit='Celsius')}\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': Inpt_weather(location='San Francisco', unit='Celsius'),\n",
       " 'history': 'Human: Hello\\nAI: Hi there! How can I assist you today?\\nHuman: Hello\\nAI: Hi there! How can I assist you today?\\nHuman: How much is the sum of 4, 234, 5766786786?\\nAI: 5766787024',\n",
       " 'response': '{\"location\": \"San Francisco\", \"temperature\": \"30\", \"unit\": \"Celsius\", \"forecast\": [\"sunny\", \"windy\"]}'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "mChain('What is the weather in San Francisco? in Celsius')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dasher_dev",
   "language": "python",
   "name": "dasher_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
